---
layout: post
title: "Testing C/C++ #2: Why T, let alone TDD?"
date: 2017-10-09
comments: true
---

Every single individual who writes code will at certain points introduce bugs. Software tends to experience a lot of change throughout its life, and so inevitably some changes will bring about unexpected behavior. These undesired behaviors are often revealed during testing.

<!--excerpt--> 

This document illustrates some of the reasoning behind testing code, as well as the potential impact of practicing techniques such as TDD.

## Types of tests

Testing can mean a number of things. For example, an individual discussing testing could be referring to unit tests, acceptance tests, integration tests, or their own personal process that is entirely unique. Let's talk a little bit about these various types of tests. These documents are mostly concerned with unit tests, so I will spend more time on that category.

**Unit tests** are typically written at the most basic level, testing discrete parts of a single function or class. These tests will often contain some arrangement of the test, an action to be taken, and an assertion to be checked at the end. Testing a constructor, for instance, would be as simple as declaring a new object and asserting that it is correctly composed. Let's look at a simple example using Catch for testing a function that adds two integers:

```cpp
SCENARIO("Add 0 and 1", "[add]") {
  GIVEN("Integers 0 and 1") {
    int a = 0;
    int b = 1;
    WHEN("They are passed into add") {
      int result = add(a, b);
      THEN("It should return 1") {
        ASSERT(1 == result);
      }
    }
  }
}
```

Assuming the function simply returns `a + b`, this unit test would pass. More tests might be written to capture other cases such as out of bounds values or negative numbers. The idea behind high unit test coverage is to accomplish two things: provide a mechanism that reports when and where something breaks, and provide executable documentation for the behavior of a function or class. Unit tests are best written throughout development to ensure good test coverage.

**Acceptance tests** are more high level than unit tests. The tests result in a binary pass/fail like unit tests, but the acceptance criteria is often defined by functionality expected by the end user. Acceptance tests can be made up entirely of unit tests, but sometimes have other higher-level requirements as well. The goal of acceptance tests is usually to pass documented requirements. Acceptance tests are typically defined early in the design process.

**Integration tests** involve testing a complete system composed of various subsystems, like a finished library or a maker board project. The goal of integration tests is to ensure subsystems work together as expected. Integration tests are usually written after the subsystems have undergone a good amount of development, and if unit tests were neglected the process can be much more difficult.

Sometimes a developer will have his or her own testing process, such as debug messages sent to a console or LED indicators. While these techniques tend to work out in practice, they are often distributed throughout the source code and can make maintainence more difficult. That said, any testing is better than none if the tests are useful.

## The significance of unit tests

So, the point of testing is to make sure something _works_, and "works" is defined based on the type of test. Of these types, good unit tests are the most effective at catching and preventing bugs. 

Let's assume no unit tests are written for a project that has just been deemed ready for test, and we want to proceed with integration testing. This project might be composed of several modules that each have a library of methods and all interact in some way. Our integration test would need to ensure each module not only does what it should, but that it continues to do so while interacting with each of the other modules. Getting good test coverage means writing an _overwhelming_ number of test cases. Due to time and budget constraints, the team only has time to write enough to cover the bare minimum to pass the acceptance criteria. The test is completed and the team moves on. During the next project, several engineers become distracted by bugs that keep showing up in the old project, and the cycle repeats itself.

If the project had been completed with good unit test coverage, each discrete piece would already have some level of confidence attached to it, as well as a mechanism in place to catch broken pieces as soon as they occur. This would allow the team to focus entirely on writing interaction cases for the integration test, catch far more bugs during the test, and confidently release a more stable product.

This brings us to the concept of TDD, or test-driven development. 

## Test-driven development

TDD is the practice of writing tests for functionality prior to implementation. In detail, the workflow follows these steps:

1) Write a very basic test for the non-existant component (a function, for instance).

2) Attempt to build and watch it fail, usually by calling the unwritten function. This proves that the test is captured by the test suite.

3) Make the test build, but ensure that it fails. So, define the function but don't write the implementation necessary for it to pass. This proves that the test is working.

4) Implement just enough functionality to make the test pass.

5) Refactor the code as needed.

6) Repeat until the function is complete.

These steps are often referred to as the [Red-Green-Refactor cycle](https://www.izenbridge.com/blog/phases-of-test-driven-development-tdd/).

This process results in a lot of tests for our function. Each repetition adds a single piece of functionality and a test to go with it, so the resulting function is entirely covered.

**NOTE: A common misconception here is that a function with missing test cases is no better for preventing bugs than a function with no tests at all. Not so. During any development, functionality is limited to what the developer has implemented, so including tests for the implementation proves that it behaves as the developer _intended_. Expecting code to do more than that is simply unreasonable.**

## Impact of TDD

[James Grenning](http://blog.wingman-sw.com/) wrote an interesting article titled [Physics of Test Driven Development](http://blog.wingman-sw.com/archives/16) that discusses the impact of TDD on the time spent diagnosing a bug after it's been discovered. Since bugs are not typically discovered immediately after they've been introduced, James asserts that the amount of time spent diagnosing a bug is proportional to the amount of time elapsed between its introduction and discovery.

The rapid feedback provided by TDD allows bugs to be caught much quicker, as they often trigger a failing test shortly after they are introduced. Assuming the amount of time necessary to fix a bug after it has been discovered is the same regardless of development technique, reducing the time spent finding a bug to a fixed interval significantly reduces overall debug time.

Let's assume there are two developers who would like to complete similar projects, and they both tend to introduce bugs to their code at the same average rate of one bug per day. Let's also assume it takes them about ten minutes to fix a bug after it's been found, and the time they need to diagnose it is equivalent to the amount of time spent developing since it was introduced.

Now, let's assume developer A practices TDD with 10-minute test intervals, and developer B practices what James refers to as Debug Later Programming (DLP), or testing a feature after it has been written, at 60-minute test intervals. 

If both developers introduce a bug to their project after five minutes of work, by the time the bug is discovered it will take developer A approximately 15 minutes total to find and fix it. Developer B will take about 65 minutes to do the same. In an eight hour day, developer A will have spent about 3% of his day debugging, while developer B will have spent 13.5%. The time increases multiplicatively with additional bugs, to the point where 4 bugs would take up more than half of developer B's day but only an eighth of developer A's.

I know, that's a lot of numbers for a short paragraph. Here's a visualization:

![TDD vs DLP]({{ "/assets/img/2017-10-09-testing-2.png" }})

This may seem exaggerated, and perhaps it is for most cases. But some bugs can take hours, even days to sort out, particularly if they seem to have come out of nowhere. Sometimes they require some significant refactoring. My point is, it can realistically be a lot worse than this. Driving with tests ensures as much coverage and clarity as possible in order to minimize the overhead of debugging.

I hope this has clarified some of these concepts. TDD can be difficult to wrap one's head around, and certainly isn't for everybody. But there are good reasons to consider practicing it.